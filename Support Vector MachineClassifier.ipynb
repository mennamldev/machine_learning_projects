{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyOqZfUAIp8KqkpWzrPJ/S87"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["SVM classifier"],"metadata":{"id":"GPzWd1dL3E86"}},{"cell_type":"code","execution_count":1,"metadata":{"id":"BD0mii2G1sMk","executionInfo":{"status":"ok","timestamp":1736741585955,"user_tz":-60,"elapsed":373,"user":{"displayName":"Mennatullah Ibrahim","userId":"12503366848613211978"}}},"outputs":[],"source":["# importing libraries\n","import numpy as np\n"]},{"cell_type":"code","source":["# SVM classifier\n","class SVM_classifier():\n","\n","# creating the hyperparameters\n","  def __init__(self, learning_rate, no_of_iterations, lamda_parameter):\n","\n","      self.learning_rate = learning_rate\n","      self.no_of_iterations = no_of_iterations\n","      self.lamda_parameter = lamda_parameter\n","# fitting the dataset to SVM classifier\n","  def fit(self, x, y):\n","      # m : number of data points (rows), n: number of input features(columns)\n","      self.m, self.n = x.shape\n","\n","      # initiating the weight and bias values\n","      self.w = np.zeros(self.n)\n","      self.b = 0\n","      self.x = x\n","      self.y = y\n","\n","      # initiating the gradient descent algorithem for optimization\n","      for i in range(self.no.of.iterations):\n","          self.update_weights()\n","\n","# updating weight and bias value\n","  def update_weights(self):\n","    # label encoding\n","    y_label = np.where(self.y  <= 0, -1, 1) # if the value is 0 it will be converted to -1, if the value is 1 it will stay the same.\n","\n","    # gradiant dw, db\n","    for index, x_i in enumerate(self.x): # enumerate is used as a counter\n","        condition = y_label[index] * (np.dot(x_i, self.w) - self.b) >= 1\n","\n","        if (condition == True):\n","            dw = 2 * self.lamda_parameter * self.w\n","            db = 0\n","        else:\n","           dw = 2 * self.lamda_perameter * self.w - np.dot(x_i, y_label[index])\n","           db = y_label[index]\n","\n","        self.w = self.w - self.learning_rate * dw\n","        self.b = self.b - self.learning_rate * db\n","# predict the label for a given value\n","  def predict(self, x):\n","\n","    output = np.dot(x, self.w) - self.b\n","    predic_labels = np.sign(output) # to find if the number is positive or negetive\n","\n","    y_hat = np.where(predic_labels <= -1, 0, 1)\n","\n","    return y_hat()\n","\n"],"metadata":{"id":"QeOKCbX53QaR"},"execution_count":null,"outputs":[]}]}